{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515d5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "#importing the required libraries into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebd9e90",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data. This task will be done in following steps:\n",
    "\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0863a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_required</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>KrazyBee</td>\n",
       "      <td>Bangalore/Bengaluru(Old Madras Road)</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6-11 Yrs</td>\n",
       "      <td>Global Indian School Education Services</td>\n",
       "      <td>Bangalore/Bengaluru, Pune</td>\n",
       "      <td>Sr. Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Master Data Management Business Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5-7 Yrs</td>\n",
       "      <td>Optum</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>FullStackTechies</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Intern Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>iMindYourBusiness</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>Data Analyst - Python/Artificial Intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0-2 Yrs</td>\n",
       "      <td>Bright Money</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>Info Origin Inc.</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Leap COE Intern(Data Analyst)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Concentrix</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram</td>\n",
       "      <td>Forecasting Analyst/ Data Scientist (US Client)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experience_required                             company_name  \\\n",
       "0             3-6 Yrs                                 KrazyBee   \n",
       "1            6-11 Yrs  Global Indian School Education Services   \n",
       "2             6-8 Yrs                                Accenture   \n",
       "3             5-7 Yrs                                    Optum   \n",
       "4             0-1 Yrs                         FullStackTechies   \n",
       "5             0-2 Yrs                        iMindYourBusiness   \n",
       "6             0-2 Yrs                             Bright Money   \n",
       "7             2-5 Yrs                                    Bayer   \n",
       "8             0-1 Yrs                         Info Origin Inc.   \n",
       "9             3-8 Yrs                               Concentrix   \n",
       "\n",
       "                                        job_location  \\\n",
       "0               Bangalore/Bengaluru(Old Madras Road)   \n",
       "1                          Bangalore/Bengaluru, Pune   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9              Bangalore/Bengaluru, Gurgaon/Gurugram   \n",
       "\n",
       "                                         job_title  \n",
       "0                              Senior Data Analyst  \n",
       "1                                 Sr. Data Analyst  \n",
       "2          Master Data Management Business Analyst  \n",
       "3                              Senior Data Analyst  \n",
       "4                              Intern Data Analyst  \n",
       "5    Data Analyst - Python/Artificial Intelligence  \n",
       "6                                     Data Analyst  \n",
       "7                                     Data Analyst  \n",
       "8                    Leap COE Intern(Data Analyst)  \n",
       "9  Forecasting Analyst/ Data Scientist (US Client)  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "#Entering Data into Search Bars :-\n",
    "search_field_designation=driver.find_element(By.CLASS_NAME, \"suggestor-input\") \n",
    "search_field_location=driver.find_element(By.XPATH, '/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input') \n",
    "search_field_designation.send_keys(\"Data Analyst\")\n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "#Clicking the search button:-\n",
    "search_button=driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search_button.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Creating empty lists for scraping data and use the scraped data to make DataFrame:-\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "\n",
    "\n",
    "#Scraping the job-titles:-\n",
    "title=driver.find_elements(By.XPATH, \"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the job-location:-\n",
    "location=driver.find_elements(By.XPATH, \"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for i in location[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the company_name:-\n",
    "company=driver.find_elements(By.XPATH, \"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the experience_required:-\n",
    "experience=driver.find_elements(By.XPATH, \"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for i in experience[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)\n",
    "time.sleep(3)\n",
    "\n",
    "# creating the dataframe from the scraped data and taking only first 10 jobs:-\n",
    "JOBS=pd.DataFrame({\"experience_required\":experience_required[0:10],\"company_name\":company_name[0:10],\"job_location\":job_location[0:10],\n",
    "                \"job_title\":job_title[0:10]})\n",
    "JOBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0afc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17f8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72302b19",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in\n",
    "“Bangalore” location. You have to scrape the job-title, job-location,\n",
    "company_name, full job-description. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter\n",
    "“Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65046cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_required</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "      <td>Bangalore/Bengaluru, Chennai, Mumbai (All Areas)</td>\n",
       "      <td>Tcs Hiring For Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5-12 Yrs</td>\n",
       "      <td>PwC</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>Senior Manager - EmTech - Machine Learning - P&amp;T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5-9 Yrs</td>\n",
       "      <td>CitiusTech</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune</td>\n",
       "      <td>Assistant Manager - Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9-14 Yrs</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, Indore, New...</td>\n",
       "      <td>Data scientist _Tata Consultancy Services(Tcs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...</td>\n",
       "      <td>Job||Job Opening For AI Technologist - Data Sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4-9 Yrs</td>\n",
       "      <td>NTT DATA Business Solutions Private Limited</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>Hiring For DATA Scientist @ NTT DATA Business ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11-20 Yrs</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...</td>\n",
       "      <td>Urgent Job Opening For AI Practitioner - Data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7-12 Yrs</td>\n",
       "      <td>Concentrix</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Concentrix</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram</td>\n",
       "      <td>Forecasting Analyst/ Data Scientist (US Client)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experience_required                                 company_name  \\\n",
       "0             3-8 Yrs              TATA CONSULTANCY SERVICES (TCS)   \n",
       "1            5-12 Yrs                                          PwC   \n",
       "2             5-9 Yrs                                   CitiusTech   \n",
       "3             6-8 Yrs                                    Accenture   \n",
       "4            9-14 Yrs              TATA CONSULTANCY SERVICES (TCS)   \n",
       "5            5-10 Yrs                                        Wipro   \n",
       "6             4-9 Yrs  NTT DATA Business Solutions Private Limited   \n",
       "7           11-20 Yrs                                        Wipro   \n",
       "8            7-12 Yrs                                   Concentrix   \n",
       "9             3-8 Yrs                                   Concentrix   \n",
       "\n",
       "                                        job_location  \\\n",
       "0   Bangalore/Bengaluru, Chennai, Mumbai (All Areas)   \n",
       "1  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...   \n",
       "2                  Bangalore/Bengaluru, Mumbai, Pune   \n",
       "3  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...   \n",
       "4  Bangalore/Bengaluru, Kochi/Cochin, Indore, New...   \n",
       "5  Bangalore/Bengaluru, New Delhi, Hyderabad/Secu...   \n",
       "6  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...   \n",
       "7  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9              Bangalore/Bengaluru, Gurgaon/Gurugram   \n",
       "\n",
       "                                           job_title  \n",
       "0                      Tcs Hiring For Data Scientist  \n",
       "1   Senior Manager - EmTech - Machine Learning - P&T  \n",
       "2                   Assistant Manager - Data Science  \n",
       "3                   Analystics & Modeling Specialist  \n",
       "4     Data scientist _Tata Consultancy Services(Tcs)  \n",
       "5  Job||Job Opening For AI Technologist - Data Sc...  \n",
       "6  Hiring For DATA Scientist @ NTT DATA Business ...  \n",
       "7  Urgent Job Opening For AI Practitioner - Data ...  \n",
       "8                                     Data Scientist  \n",
       "9    Forecasting Analyst/ Data Scientist (US Client)  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# entering Data into Search Bars :-\n",
    "search_field_designation=driver.find_element(By.CLASS_NAME, \"suggestor-input\") \n",
    "search_field_location=driver.find_element(By.XPATH, '/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input') \n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "search_field_location.send_keys(\"Bangalore\")\n",
    "\n",
    "# clicking the search button:-\n",
    "search_button=driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search_button.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# creating empty lists for scraping data and use the scraped data to make DataFrame:-\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "\n",
    "#scraping the job-titles:-\n",
    "title=driver.find_elements(By.XPATH, \"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the job-location:-\n",
    "location=driver.find_elements(By.XPATH, \"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for i in location[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the company_name:-\n",
    "company=driver.find_elements(By.XPATH, \"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the experience_required:-\n",
    "experience=driver.find_elements(By.XPATH, \"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for i in experience[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)\n",
    "time.sleep(3)\n",
    "\n",
    "# creating the dataframe from the scraped data and taking only first 10 jobs:-\n",
    "JOBS=pd.DataFrame({\"experience_required\":experience_required[0:10],\"company_name\":company_name[0:10],\"job_location\":job_location[0:10],\n",
    "                \"job_title\":job_title[0:10]})\n",
    "JOBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c99251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a9d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aeb3a1db",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the\n",
    "webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name,\n",
    "experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field .\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b78becd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_title</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wipro</td>\n",
       "      <td>New Delhi, Hyderabad/Secunderabad, Pune, Chenn...</td>\n",
       "      <td>Job||Job Opening For AI Technologist - Data Sc...</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXL</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "      <td>Data Scientist - Noida/Bangalore</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gojek</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Data Scientist - Risk Platform</td>\n",
       "      <td>5-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infosys</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gojek</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Decision Scientist</td>\n",
       "      <td>4-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Optum</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>I O Engineering Analyst</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              company_name                                       job_location  \\\n",
       "0                    Wipro  New Delhi, Hyderabad/Secunderabad, Pune, Chenn...   \n",
       "1                      EXL                         Noida, Bangalore/Bengaluru   \n",
       "2                    Gojek                                Bangalore/Bengaluru   \n",
       "3                  Infosys                                Bangalore/Bengaluru   \n",
       "4        Applied Materials                                Bangalore/Bengaluru   \n",
       "5        Applied Materials                                Bangalore/Bengaluru   \n",
       "6        Applied Materials                                Bangalore/Bengaluru   \n",
       "7                    Gojek                                Bangalore/Bengaluru   \n",
       "8  Boston Consulting Group                     New Delhi, Bangalore/Bengaluru   \n",
       "9                    Optum                                            Chennai   \n",
       "\n",
       "                                           job_title experience_required  \n",
       "0  Job||Job Opening For AI Technologist - Data Sc...            5-10 Yrs  \n",
       "1                   Data Scientist - Noida/Bangalore            5-10 Yrs  \n",
       "2                     Data Scientist - Risk Platform            5-11 Yrs  \n",
       "3                                     Data Scientist             2-7 Yrs  \n",
       "4                                     Data Scientist             2-4 Yrs  \n",
       "5                                     Data Scientist             4-7 Yrs  \n",
       "6                                     Data Scientist             2-4 Yrs  \n",
       "7                                 Decision Scientist             4-9 Yrs  \n",
       "8                    DigitalBCG GAMMA Data Scientist             2-5 Yrs  \n",
       "9                            I O Engineering Analyst             2-7 Yrs  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# entering Data into Search Bars :-\n",
    "search_field_designation=driver.find_element(By.CLASS_NAME, \"suggestor-input\") \n",
    "search_field_location=driver.find_element(By.XPATH, '/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input') \n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "search_field_location.send_keys(\"Delhi/NCR\")\n",
    "\n",
    "# clicking the search button:-\n",
    "search_button=driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search_button.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# finding the salary check box:-\n",
    "slry_box = driver.find_element(By.XPATH, \"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[2]/label/i\")\n",
    "slry_box.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# creating empty lists for scraping data and use the scraped data to make DataFrame:-\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "\n",
    "#scraping the job-titles:-\n",
    "title=driver.find_elements(By.XPATH, \"//a[@class='title fw500 ellipsis']\")\n",
    "for i in title[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the job-location:-\n",
    "location=driver.find_elements(By.XPATH, \"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for i in location[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the company_name:-\n",
    "company=driver.find_elements(By.XPATH, \"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in company[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the experience_required:-\n",
    "experience=driver.find_elements(By.XPATH, \"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for i in experience[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)\n",
    "time.sleep(3)\n",
    "\n",
    "# creating the dataframe from the scraped data and taking only first 10 jobs:-\n",
    "DELHI_NCR_JOBS=pd.DataFrame({\"company_name\":company_name[0:10],\"job_location\":job_location[0:10],\n",
    "                \"job_title\":job_title[0:10], \"experience_required\":experience_required[0:10]})\n",
    "DELHI_NCR_JOBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fdca0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d09168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dba2965f",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and \n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the \n",
    "required data as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ec99ef88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fair-x</td>\n",
       "      <td>Casual Sneakers White Shoes For Girls And Wome...</td>\n",
       "      <td>₹360</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fair-x</td>\n",
       "      <td>Stylish Casual Sports Shoe Sneakers For Women ...</td>\n",
       "      <td>₹360</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>X-Ray 2 Square Sneakers For Men  (White)</td>\n",
       "      <td>₹799</td>\n",
       "      <td>20% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>Sneakers For Men  (Black)</td>\n",
       "      <td>₹264</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>JFWHUNTER SUEDE WINTER WHITE NOOS IN Sneakers ...</td>\n",
       "      <td>₹298</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>Mirrored, Riding Glasses, UV Protection Sports...</td>\n",
       "      <td>₹360</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Sports Sunglasses (Free Size)</td>\n",
       "      <td>₹949</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hamiw</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹189</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹249</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Dannilo</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹664</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brand                                        Description Price  \\\n",
       "0          Fair-x  Casual Sneakers White Shoes For Girls And Wome...  ₹360   \n",
       "1          Fair-x  Stylish Casual Sports Shoe Sneakers For Women ...  ₹360   \n",
       "2        Fastrack           X-Ray 2 Square Sneakers For Men  (White)  ₹799   \n",
       "3       New Specs                          Sneakers For Men  (Black)  ₹264   \n",
       "4       Elligator  JFWHUNTER SUEDE WINTER WHITE NOOS IN Sneakers ...  ₹298   \n",
       "..            ...                                                ...   ...   \n",
       "95  VINCENT CHASE  Mirrored, Riding Glasses, UV Protection Sports...  ₹360   \n",
       "96      New Specs        UV Protection Sports Sunglasses (Free Size)  ₹949   \n",
       "97          Hamiw      UV Protection Wayfarer Sunglasses (Free Size)  ₹189   \n",
       "98      ROYAL SON   UV Protection Rectangular Sunglasses (Free Size)  ₹249   \n",
       "99        Dannilo                UV Protection Round Sunglasses (54)  ₹664   \n",
       "\n",
       "   Discount  \n",
       "0   75% off  \n",
       "1   75% off  \n",
       "2   20% off  \n",
       "3   89% off  \n",
       "4   88% off  \n",
       "..      ...  \n",
       "95  75% off  \n",
       "96  52% off  \n",
       "97  88% off  \n",
       "98  83% off  \n",
       "99  66% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# entering Data into Search Bars :-\n",
    "location=driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "location.send_keys('sunglasses')\n",
    "\n",
    "# clicking the search button:-\n",
    "search=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "search.click()\n",
    "\n",
    "# creating empty lists for scraping data and use the scraped data to make DataFrame:-\n",
    "brand=[]\n",
    "product_description=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "time.sleep(3)\n",
    "\n",
    "#scrapping the required details:-\n",
    "\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):#for loop for scrapping 4 page\n",
    "    brands=driver.find_elements(By.CLASS_NAME, '_2WkVRV')#scraping brands name by class name='_2WkVRV'\n",
    "    for i in brands:\n",
    "        brand.append(i.text)#appending the text in Brand list\n",
    "    desc=driver.find_elements(By.XPATH, '//a[@class=\"IRpwTa\"]')#scraping description from the xpath\n",
    "    for i in desc:\n",
    "        description.append(i.text)#appending the description in list\n",
    "    prices=driver.find_elements(By.XPATH, \"//div[@class='_30jeq3']\")# scraping the price from the xpath\n",
    "    for i in prices:\n",
    "        price.append(i.text)\n",
    "    disc=driver.find_elements(By.XPATH, \"//div[@class='_3Ay6Sb']//span\")# scraping the discount from the xpath\n",
    "    for i in disc:\n",
    "        discount.append(i.text)\n",
    "    nxt_button=driver.find_elements(By.XPATH, \"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "\n",
    "time.sleep(3)   \n",
    "\n",
    "#creating a dataframe:-\n",
    "\n",
    "Flipkart=pd.DataFrame({'Brand':brand[:100],\n",
    "                'Description':description[:100],\n",
    "                'Price':price[:100],\n",
    "                'Discount':discount[:100]})\n",
    "\n",
    "Flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b360a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60d734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79da391d",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter “iphone 11” in “Search” field . \n",
    "3. Then click the search button.\n",
    "You will reach to the below shown webpage\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd934b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Rating, Review Summary, Full Review]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# entering Data into Search Bars :-\n",
    "location=driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "location.send_keys('iphone 11')\n",
    "\n",
    "# clicking the search button:-\n",
    "submit=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "submit.click()\n",
    "\n",
    "# creating empty lists for scraping data and use the scraped data to make DataFrame:-\n",
    "rating=[]\n",
    "review_summary=[]\n",
    "full_review=[]\n",
    "\n",
    "# Taking 10 pages into consideration using for loop\n",
    "url=driver.find_elements(By.XPATH, '//div[@class=\"_4rR01T\"]')\n",
    "url[0:10]\n",
    "for i in url[0:10]:\n",
    "    print(i.get_attribute('href'))\n",
    "    for j in driver.find_elements(By.XPATH, \"//div[@class='_3LWZlK _1BLPMq']\"):\n",
    "        rating.append(j.text)\n",
    "    \n",
    "    for k in driver.find_elements(By.XPATH, \"//p[@class='_2-N8zT']\"):\n",
    "        review_summary.append(k.text)\n",
    "\n",
    "    for l in driver.find_elements(By.XPATH, \"//div[@class='t-ZTKy']/div/div\"):\n",
    "        full_review.append(l.text)\n",
    "    \n",
    "#creating a dataframe:-\n",
    "IPhone11=pd.DataFrame({'Rating':rating, 'Review Summary':full_review, 'Full Review':full_review})\n",
    "IPhone11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c026c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b3d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "692768c2",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the \n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "69bc277a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Comfortable Premium Stylish Unique Trendy Popu...</td>\n",
       "      <td>₹228</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Sneakers For Men  (Black)</td>\n",
       "      <td>₹544</td>\n",
       "      <td>71% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>ShoesForGirlsStylishLatestFashionSportsCasualC...</td>\n",
       "      <td>₹349</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Magnolia</td>\n",
       "      <td>Sneakers For Women  (Multicolor)</td>\n",
       "      <td>₹374</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KWIK FIT</td>\n",
       "      <td>Latest Collection Sneakers For Women  (White)</td>\n",
       "      <td>₹397</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Sneakers For Women  (White, Pink)</td>\n",
       "      <td>₹327</td>\n",
       "      <td>79% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Sparx</td>\n",
       "      <td>Sneakers For Women  (White)</td>\n",
       "      <td>₹756</td>\n",
       "      <td>15% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Sneakers For Men  (White)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Solid Design Casual shoes For Girl's &amp; Women's...</td>\n",
       "      <td>₹2,400</td>\n",
       "      <td>49% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Numenzo</td>\n",
       "      <td>Stylish &amp; Trending Outdoor Walking Comfortable...</td>\n",
       "      <td>₹599</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Brand                                        Description   Price  \\\n",
       "0     Layasa  Comfortable Premium Stylish Unique Trendy Popu...    ₹228   \n",
       "1     Chevit                          Sneakers For Men  (Black)    ₹544   \n",
       "2     Layasa  ShoesForGirlsStylishLatestFashionSportsCasualC...    ₹349   \n",
       "3   Magnolia                   Sneakers For Women  (Multicolor)    ₹374   \n",
       "4   KWIK FIT      Latest Collection Sneakers For Women  (White)    ₹397   \n",
       "..       ...                                                ...     ...   \n",
       "95    Chevit                  Sneakers For Women  (White, Pink)    ₹327   \n",
       "96     Sparx                        Sneakers For Women  (White)    ₹756   \n",
       "97    Chevit                          Sneakers For Men  (White)    ₹499   \n",
       "98      PUMA  Solid Design Casual shoes For Girl's & Women's...  ₹2,400   \n",
       "99   Numenzo  Stylish & Trending Outdoor Walking Comfortable...    ₹599   \n",
       "\n",
       "   Discount  \n",
       "0   77% off  \n",
       "1   71% off  \n",
       "2   65% off  \n",
       "3   62% off  \n",
       "4   80% off  \n",
       "..      ...  \n",
       "95  79% off  \n",
       "96  15% off  \n",
       "97  72% off  \n",
       "98  49% off  \n",
       "99  40% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://www.flipkart.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# entering Data into Search Bars :-\n",
    "location=driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "location.send_keys('sneakers')\n",
    "\n",
    "# clicking the search button:-\n",
    "submit=driver.find_element(By.CLASS_NAME,\"L0Z3Pu\")\n",
    "submit.click()\n",
    "\n",
    "# creating empty lists for scraping data and use the scraped data to make DataFrame:-\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scrapping the required details:-\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):#for loop for scrapping 4 page\n",
    "    brands=driver.find_elements(By.CLASS_NAME, '_2WkVRV')#scraping brands name by class name='_2WkVRV'\n",
    "    for i in brands:\n",
    "        brand.append(i.text)#appending the text in Brand list\n",
    "    prices=driver.find_elements(By.XPATH, \"//div[@class='_30jeq3']\")# scraping the price from the xpath\n",
    "    for i in prices:\n",
    "        price.append(i.text)\n",
    "    disc=driver.find_elements(By.XPATH, \"//div[@class='_3Ay6Sb']//span\")# scraping the discount from the xpath\n",
    "    for i in disc:\n",
    "        discount.append(i.text)\n",
    "    nxt_button=driver.find_elements(By.XPATH, \"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "        \n",
    "        \n",
    "        \n",
    "time.sleep(2)\n",
    "\n",
    "# Since in some records not getting description so try from inside of urls\n",
    "urls = []\n",
    "\n",
    "for page in range(start,end):#for loop for scrapping 4 page\n",
    "    \n",
    "    product_url = driver.find_elements(By.XPATH, \"//a[@class='_2UzuFa']\")\n",
    "    for i in product_url:\n",
    "        urls.append(i.get_attribute('href'))\n",
    "    \n",
    "    nxt_button=driver.find_elements(By.XPATH, \"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "        \n",
    "time.sleep(2)\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    desc=driver.find_elements(By.XPATH, '//span[@class=\"B_NuCI\"]')#scraping description from the xpath\n",
    "    for i in desc:\n",
    "        description.append(i.text)#appending the description in list\n",
    "time.sleep(2)        \n",
    "time.sleep(3)\n",
    "\n",
    "#creating a dataframe:-\n",
    "Sneakers=pd.DataFrame({'Brand':brand[:100],\n",
    "                'Description':description[:100],\n",
    "                'Price':price[:100],\n",
    "                'Discount':discount[:100]})\n",
    "\n",
    "Sneakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0132f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c983df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "112eba9b",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set second Price filter and Color filter to “Black”, as shown in the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe \n",
    "description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://www.myntra.com/shoes\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "#clicking on price filter:-\n",
    "price_button = driver.find_element(By.XPATH, \"element\")\n",
    "price_button.click()\n",
    "\n",
    "#clicking on black color filter:-\n",
    "black_color_button = driver.find_element(By.XPATH, \"element\")\n",
    "black_color_button.click()\n",
    "\n",
    "\n",
    "#creating empty lists:-\n",
    "shoe_names=[]\n",
    "s_desc=[]\n",
    "short_desc=[]\n",
    "price=[]\n",
    "page_urls = []\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "# scrape next pages urls:-\n",
    "nxt_page = driver.find_elements(By.XPATH, \"element\")\n",
    "for i in nxt_page:\n",
    "    page_urls.append(i.get_attribute('href'))\n",
    "    \n",
    "\n",
    "for url in page_urls[:3]:\n",
    "    driver.get(url)\n",
    "    Names=driver.find_elements(By.XPATH, \"ELEMENT\")  #for scrapping shoe brand names\n",
    "    for i in Names:\n",
    "        shoe_names.append(i.text)\n",
    "    \n",
    "    desc=driver.find_elements(By.XPATH, \"ELEMENT\") #for scrapping shoe short-description\n",
    "    for i in desc:\n",
    "        s_desc.append(i.text)\n",
    "    for j in range(0,len(s_desc),2):\n",
    "        short_desc.append(s_desc[j])\n",
    "    \n",
    "    rs=driver.find_elements(By.XPATH, \"ELEMENT\")  #for scrapping shoe prices\n",
    "    for i in rs:\n",
    "        price.append(i.text)    \n",
    "        \n",
    "Shoes=pd.DataFrame({'Brand': shoe_names[0:100],'Short-description': short_desc[0:100],'Price': price[0:100]})\n",
    "Shoes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6af72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd5681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ddac922",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a026dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 53>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m         ratings\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO rating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#appending the No rating if no rating is there\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m#creating a dataframe:-\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m Laptop\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mTitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mprice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRatings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mRatings\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m Laptop\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    672\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    678\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    679\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "#locating search bar:-\n",
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "location.send_keys('laptop')\n",
    "search=driver.find_element(By.CLASS_NAME,\"nav-search-submit\")\n",
    "search.click()\n",
    "\n",
    "#creating the empty list:-\n",
    "Title=[]\n",
    "Ratings=[]\n",
    "price=[]\n",
    "\n",
    "#locating the core i7 filter:-\n",
    "filter_button=driver.find_elements(By.XPATH, \"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in filter_button:\n",
    "    if i.text=='Intel Core i7':\n",
    "        i.click()\n",
    "        break\n",
    "\n",
    "#Scrapping Titles:-\n",
    "titles=driver.find_elements(By.XPATH, \"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "for i in titles[:10]:\n",
    "    Title.append(i.text)\n",
    "    \n",
    "    \n",
    "#scrapping Price:-\n",
    "prices=driver.find_elements(By.XPATH, \"//span[@class='a-price-whole']\")\n",
    "for i in prices[:10]:\n",
    "    price.append(i.text)\n",
    "\n",
    "#locating Ratings:-\n",
    "urls=driver.find_elements(By.XPATH, \"//a[@class='a-link-normal a-text-normal']\")#collecting urls of all the laptop\n",
    "UR=[]\n",
    "for i in urls[:10]:\n",
    "    UR.append(i.get_attribute('href'))#getting the url of first 10 laptops\n",
    "for url in UR:#loop for every laptop in the list\n",
    "    driver.get(url)\n",
    "    try:                  #exception handling for nosuchelementexception                                                    #click the rating link found\n",
    "        rating=driver.find_element(By.XPATH, \"//span[@class='a-size-base a-nowrap']//span\")#locating the rating\n",
    "        ratings.append(i.text)#appending the ratings in Ratings list\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        ratings.append(\"NO rating\")#appending the No rating if no rating is there\n",
    "        \n",
    "#creating a dataframe:-\n",
    "Laptop=pd.DataFrame({\"Title\":Title, \"Price\":price, \"Ratings\":Ratings})\n",
    "Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9701ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f915c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f752db3d",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida \n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company. \n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "“Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4234b696",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//title[@class='Location']\"}\n  (Session info: chrome=104.0.5112.81)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x01165FD3+2187219]\n\tOrdinal0 [0x010FE6D1+1763025]\n\tOrdinal0 [0x01013E78+802424]\n\tOrdinal0 [0x01041C10+990224]\n\tOrdinal0 [0x01041EAB+990891]\n\tOrdinal0 [0x0106EC92+1174674]\n\tOrdinal0 [0x0105CBD4+1100756]\n\tOrdinal0 [0x0106CFC2+1167298]\n\tOrdinal0 [0x0105C9A6+1100198]\n\tOrdinal0 [0x01036F80+946048]\n\tOrdinal0 [0x01037E76+949878]\n\tGetHandleVerifier [0x014090C2+2721218]\n\tGetHandleVerifier [0x013FAAF0+2662384]\n\tGetHandleVerifier [0x011F137A+526458]\n\tGetHandleVerifier [0x011F0416+522518]\n\tOrdinal0 [0x01104EAB+1789611]\n\tOrdinal0 [0x011097A8+1808296]\n\tOrdinal0 [0x01109895+1808533]\n\tOrdinal0 [0x011126C1+1844929]\n\tBaseThreadInitThunk [0x7660FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77137A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77137A6E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [109]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m search_button\u001b[38;5;241m=\u001b[39mdriver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/html/body/div[1]/div/div/div[2]/div[1]/div[1]/div/div/div/button/span\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m search_button\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m---> 14\u001b[0m search_location\u001b[38;5;241m=\u001b[39m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//title[@class=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m search_location\u001b[38;5;241m.\u001b[39mclick()    \n\u001b[0;32m     16\u001b[0m search_field_designation\u001b[38;5;241m=\u001b[39mdriver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata-v-3afdbeb9\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:857\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    854\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    855\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m value\n\u001b[1;32m--> 857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:435\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    433\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(\n\u001b[0;32m    437\u001b[0m         response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:247\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    245\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//title[@class='Location']\"}\n  (Session info: chrome=104.0.5112.81)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x01165FD3+2187219]\n\tOrdinal0 [0x010FE6D1+1763025]\n\tOrdinal0 [0x01013E78+802424]\n\tOrdinal0 [0x01041C10+990224]\n\tOrdinal0 [0x01041EAB+990891]\n\tOrdinal0 [0x0106EC92+1174674]\n\tOrdinal0 [0x0105CBD4+1100756]\n\tOrdinal0 [0x0106CFC2+1167298]\n\tOrdinal0 [0x0105C9A6+1100198]\n\tOrdinal0 [0x01036F80+946048]\n\tOrdinal0 [0x01037E76+949878]\n\tGetHandleVerifier [0x014090C2+2721218]\n\tGetHandleVerifier [0x013FAAF0+2662384]\n\tGetHandleVerifier [0x011F137A+526458]\n\tGetHandleVerifier [0x011F0416+522518]\n\tOrdinal0 [0x01104EAB+1789611]\n\tOrdinal0 [0x011097A8+1808296]\n\tOrdinal0 [0x01109895+1808533]\n\tOrdinal0 [0x011126C1+1844929]\n\tBaseThreadInitThunk [0x7660FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77137A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77137A6E+238]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "driver.get(\"https://www.ambitionbox.com\")\n",
    "\n",
    "#Entering Data into Search Bars :-\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/nav[2]/div/ul/li[5]/a\")\n",
    "search.click()\n",
    "search_field_designation=driver.find_element(By.XPATH, \"/html/body/div[1]/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input\") \n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "search_button=driver.find_element(By.XPATH, \"/html/body/div[1]/div/div/div[2]/div[1]/div[1]/div/div/div/button/span\")\n",
    "search_button.click()\n",
    "search_location=driver.find_element(By.XPATH, \"//title[@class='Location']\")\n",
    "search_location.click()    \n",
    "search_field_designation=driver.find_element(By.CLASS_NAME, \"data-v-3afdbeb9\") \n",
    "search_field_designation.send_keys(\"Noida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0789e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb8316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c81d8452",
   "metadata": {},
   "source": [
    "Q10: Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary. \n",
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and \n",
    "then click on “Data Scientist”.\n",
    "You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average \n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf6e00d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[1]/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/div/div[2]/div[2]/div[2]/div/p\"}\n  (Session info: chrome=104.0.5112.81)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00B85FD3+2187219]\n\tOrdinal0 [0x00B1E6D1+1763025]\n\tOrdinal0 [0x00A33E78+802424]\n\tOrdinal0 [0x00A61C10+990224]\n\tOrdinal0 [0x00A61EAB+990891]\n\tOrdinal0 [0x00A8EC92+1174674]\n\tOrdinal0 [0x00A7CBD4+1100756]\n\tOrdinal0 [0x00A8CFC2+1167298]\n\tOrdinal0 [0x00A7C9A6+1100198]\n\tOrdinal0 [0x00A56F80+946048]\n\tOrdinal0 [0x00A57E76+949878]\n\tGetHandleVerifier [0x00E290C2+2721218]\n\tGetHandleVerifier [0x00E1AAF0+2662384]\n\tGetHandleVerifier [0x00C1137A+526458]\n\tGetHandleVerifier [0x00C10416+522518]\n\tOrdinal0 [0x00B24EAB+1789611]\n\tOrdinal0 [0x00B297A8+1808296]\n\tOrdinal0 [0x00B29895+1808533]\n\tOrdinal0 [0x00B326C1+1844929]\n\tBaseThreadInitThunk [0x7711FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77B37A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77B37A6E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m search_designation\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Scientist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m search_designation\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m---> 15\u001b[0m select_search\u001b[38;5;241m=\u001b[39m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/html/body/div[1]/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/div/div[2]/div[2]/div[2]/div/p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m select_search\u001b[38;5;241m.\u001b[39mclick()\n\u001b[0;32m     17\u001b[0m search_button\u001b[38;5;241m=\u001b[39mdriver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas-btn-medium\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:857\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    854\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    855\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m value\n\u001b[1;32m--> 857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:435\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    433\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(\n\u001b[0;32m    437\u001b[0m         response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:247\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    245\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[1]/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/div/div[2]/div[2]/div[2]/div/p\"}\n  (Session info: chrome=104.0.5112.81)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00B85FD3+2187219]\n\tOrdinal0 [0x00B1E6D1+1763025]\n\tOrdinal0 [0x00A33E78+802424]\n\tOrdinal0 [0x00A61C10+990224]\n\tOrdinal0 [0x00A61EAB+990891]\n\tOrdinal0 [0x00A8EC92+1174674]\n\tOrdinal0 [0x00A7CBD4+1100756]\n\tOrdinal0 [0x00A8CFC2+1167298]\n\tOrdinal0 [0x00A7C9A6+1100198]\n\tOrdinal0 [0x00A56F80+946048]\n\tOrdinal0 [0x00A57E76+949878]\n\tGetHandleVerifier [0x00E290C2+2721218]\n\tGetHandleVerifier [0x00E1AAF0+2662384]\n\tGetHandleVerifier [0x00C1137A+526458]\n\tGetHandleVerifier [0x00C10416+522518]\n\tOrdinal0 [0x00B24EAB+1789611]\n\tOrdinal0 [0x00B297A8+1808296]\n\tOrdinal0 [0x00B29895+1808533]\n\tOrdinal0 [0x00B326C1+1844929]\n\tBaseThreadInitThunk [0x7711FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77B37A9E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77B37A6E+238]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Admin\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver=webdriver.Chrome(r\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "driver.get(\"https://www.ambitionbox.com\")\n",
    "\n",
    "#Entering Data into Search Bars :-\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/nav[2]/div/ul/li[3]\")\n",
    "search.click()\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[1]/nav[2]/div/ul/li[3]/div/ul/li[1]/div/div[2]/a\")\n",
    "search.click()\n",
    "search_designation=driver.find_element(By.XPATH, \"/html/body/div[1]/div/div/div[2]/div/div/div/div[1]/span/input\")\n",
    "search_designation.send_keys(\"Data Scientist\")\n",
    "search_designation.click()\n",
    "select_search=driver.find_element(By.XPATH, \"/html/body/div[1]/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/div/div[2]/div[2]/div[2]/div/p\")\n",
    "select_search.click()\n",
    "search_button=driver.find_element(By.CLASS_NAME, \"ctas-btn-medium\")\n",
    "search_button.click()\n",
    "\n",
    "#creating the empty list:-\n",
    "company_name=[]\n",
    "number_of_salaries=[]\n",
    "average_salary=[]\n",
    "min_salary=[]\n",
    "max_salary=[]\n",
    "experience_required=[]\n",
    "\n",
    "#Scraping the company name:-\n",
    "company=driver.find_elements(By.XPATH, \"//a[@class='TCS Data Scientist Salary']\")\n",
    "for i in company[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(title)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the salaries:-\n",
    "salaries=driver.find_elements(By.XPATH, \"//li[@class='salary-datapoints']/span[1]\")\n",
    "for i in salaries[0:10]:\n",
    "    salaries=i.text\n",
    "    number_of_salaries.append(location)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the salaries:-\n",
    "average_salary=driver.find_elements(By.XPATH, \"//p[@class='averageCtc']\")\n",
    "for i in average_salary[0:10]:\n",
    "    average_salary=i.text\n",
    "    average_salary.append(location)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the min salary:-\n",
    "min_salary=driver.find_elements(By.XPATH, \"//div[@class='value body-medium']/div[1]\")\n",
    "for i in min_salary[0:10]:\n",
    "    min_salary=i.text\n",
    "    min_salary.append(company)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the max salary:-\n",
    "max_salary=driver.find_elements(By.XPATH, \"//div[@class='value body-medium']/div[2]\")\n",
    "for i in max_salary[0:10]:\n",
    "    max_salary=i.text\n",
    "    max_salary.append(company)\n",
    "time.sleep(3)\n",
    "\n",
    "#scraping the experience_required:-\n",
    "experience=driver.find_elements(By.XPATH, \"//li[@class='data-v-b24b8478']/span[1]\")\n",
    "for i in experience[0:10]:\n",
    "    experience=i.text\n",
    "    experience_required.append(experience)\n",
    "time.sleep(3)\n",
    "\n",
    "#creating a dataframe:-\n",
    "Laptop=pd.DataFrame({\"Company Name\":company_name, \"Number of Salaires\":number_of_salaries, \"Average Salary\":average_salary, \"Min Salary\":min_salary, \"Max Salary\":max_salary, \"Experience Required\":experience_required})\n",
    "Laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dabd45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
